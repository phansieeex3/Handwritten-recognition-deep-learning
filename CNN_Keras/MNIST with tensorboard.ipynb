{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME_FOLDER = \"/Users/luna/workspace/TensorFlow-Tutorial/\"\n",
    "\n",
    "#create train folders\n",
    "MNIST_FOLDER = os.path.join(HOME_FOLDER, \"tf_mnist\")\n",
    "TRAIN_1_FOLDER = os.path.join(MNIST_FOLDER, \"train-1\")\n",
    "TRAIN_2_FOLDER = os.path.join(MNIST_FOLDER, \"train-2\")\n",
    "TRAIN_3_FOLDER = os.path.join(MNIST_FOLDER, \"train-3\")\n",
    "if not os.path.isdir(MNIST_FOLDER):\n",
    "    os.mkdir(MNIST_FOLDER)\n",
    "if not os.path.isdir(TRAIN_1_FOLDER):\n",
    "    os.mkdir(TRAIN_1_FOLDER)\n",
    "if not os.path.isdir(TRAIN_2_FOLDER):\n",
    "    os.mkdir(TRAIN_2_FOLDER)\n",
    "if not os.path.isdir(TRAIN_3_FOLDER):\n",
    "    os.mkdir(TRAIN_3_FOLDER)\n",
    "    \n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import urllib\n",
    "\n",
    "if sys.version_info[0] >= 3:\n",
    "    from urllib.request import urlretrieve\n",
    "else:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "GITHUB_URL ='https://raw.githubusercontent.com/mamcgrath/TensorBoard-TF-Dev-Summit-Tutorial/master/'\n",
    "    \n",
    "    \n",
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=MNIST_FOLDER\n",
    "                                                       + '/data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "urlretrieve(GITHUB_URL + 'labels_1024.tsv', MNIST_FOLDER + '/labels_1024.tsv')\n",
    "urlretrieve(GITHUB_URL + 'sprite_1024.png', MNIST_FOLDER + '/sprite_1024.png')\n",
    "\n",
    "\n",
    "#Setting up layers in CNN and training\n",
    "def conv_layer(x, channels_in, channels_out, name=\"conv\"):\n",
    "    ###########################################\n",
    "    #@TODO: Enter Code Here - use the name\n",
    "    # paramatere from the function\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "    ###########################################\n",
    "    \n",
    "        # add a name attribute to w\n",
    "        w = tf.Variable(tf.zeros([5, 5, channels_in, \n",
    "                                 channels_out]), name=\"W\")\n",
    "    \n",
    "        # add a name attribute to b\n",
    "        b = tf.Variable(tf.zeros([channels_out]), name=\"B\")\n",
    "        \n",
    "        conv = tf.nn.conv2d(x, w, \n",
    "                            strides=[1, 1, 1, 1], \n",
    "                            padding=\"SAME\")\n",
    "        \n",
    "        act = tf.nn.relu(conv + b)\n",
    "        \n",
    "        ###########################################\n",
    "        #@TODO: Enter Code Here - add histogram\n",
    "        # summaries to w, b and act, with names\n",
    "        # \"weights\", \"biases\" and \"activations\"\n",
    "        # respectively\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        ###########################################\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], \n",
    "                              strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "# Add a name attribute to fc_layer. name it \"fc\"\n",
    "def fc_layer(x, channels_in, channels_out, name=\"fc\"):\n",
    "    ###########################################\n",
    "    #@TODO: Enter Code Here - use the name\n",
    "    # paramatere from the function\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "    ###########################################\n",
    "    \n",
    "        #@TODO: add a name attribute to w. name it \"W\"\n",
    "        w = tf.Variable(tf.zeros([channels_in, \n",
    "                                  channels_out]), name=\"W\")\n",
    "        \n",
    "        #@TODO: add a name attribute to b. name it \"B\"\n",
    "        b = tf.Variable(tf.zeros([channels_out]), name=\"B\")\n",
    "        \n",
    "        act = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "        \n",
    "        ###########################################\n",
    "        #@TODO: Enter Code Here - add histogram\n",
    "        # summaries to w, b and act, with names\n",
    "        # \"weights\", \"biases\" and \"activations\"\n",
    "        # respectively\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        ###########################################\n",
    "        return act\n",
    "\n",
    "# Add a name attribute to fc_layer. name it \"fc_no_relu\"\n",
    "def fc_no_relu(x, channels_in, channels_out, name=\"fc_no_relu\"):\n",
    "    ###########################################\n",
    "    #@TODO: Enter Code Here - use the name\n",
    "    # paramatere from the function\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "    ###########################################\n",
    "    \n",
    "        #@TODO: add a name attribute to w. name it \"W\"\n",
    "        w = tf.Variable(tf.zeros([channels_in, \n",
    "                                  channels_out]), name=\"W\")\n",
    "        \n",
    "        #@TODO: add a name attribute to b. name it \"B\"\n",
    "        b = tf.Variable(tf.zeros([channels_out]), name=\"B\")\n",
    "        \n",
    "        \n",
    "        out = tf.matmul(x, w) + b\n",
    "        \n",
    "        ###########################################\n",
    "        #@TODO: Enter Code Here - add histogram\n",
    "        # summaries to w, b and act, with names\n",
    "        # \"weights\", \"biases\" and \"activations\"\n",
    "        # respectively\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        ###########################################\n",
    "        return out\n",
    "\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc,\n",
    "                num_iterations, batch_size, hparam, log_dir):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Setup placeholders, and reshape the data\n",
    "    #@TODO: Add name attribute to x. name it \"x\"\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    \n",
    "    #@TODO: Add name attribute to y. name it \"labels\"\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "    ###########################################\n",
    "    #@TODO: Enter Code Here - add image summary\n",
    "    # of x_image using tf.summary.image.\n",
    "    # name it \"input\"\n",
    "    tf.summary.image('input', x_image, 3)\n",
    "    ###########################################\n",
    "    \n",
    "    if use_two_conv:\n",
    "        #@TODO: pass the name \"conv1\"\n",
    "        conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "        \n",
    "        #@TODO: pass the name \"conv2\"\n",
    "        conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    else:\n",
    "        \n",
    "        #@TODO: pass the name \"conv\"\n",
    "        conv1 = conv_layer(x_image, 1, 64, \"conv\")\n",
    "        conv_out = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], \n",
    "                                  strides=[1, 2, 2, 1], \n",
    "                                  padding=\"SAME\")\n",
    "\n",
    "    flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    if use_two_fc:\n",
    "        \n",
    "        #@TODO: pass the name \"fc1\"\n",
    "        fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "        \n",
    "        embedding_input = fc1\n",
    "        embedding_size = 1024\n",
    "        \n",
    "        # add dropout\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "        \n",
    "        #@TODO: pass the name \"fc2\"\n",
    "        logits = fc_no_relu(fc1, 1024, 10, \"fc2\")\n",
    "        \n",
    "    else:\n",
    "        embedding_input = flattened\n",
    "        embedding_size = 7*7*64\n",
    "        \n",
    "        #@TODO: pass the name \"fc\"\n",
    "        logits = fc_no_relu(flattened, 7*7*64, 10, \"fc\")\n",
    "        logits = tf.nn.dropout(logits, keep_prob)\n",
    "    \n",
    "    ###########################################\n",
    "    #@TODO: Enter Code Here - add a name scope\n",
    "    #to xent. name it \"xent\"\n",
    "    \n",
    "    with tf.name_scope(\"xent\"):\n",
    "    ###########################################\n",
    "    \n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y), name=\"xent\")\n",
    "        \n",
    "        ###########################################\n",
    "        #@TODO: Enter Code Here - add scalar\n",
    "        # summaries to xent, with name \"xent\"\n",
    "        tf.summary.scalar(\"loss\", xent)\n",
    "        ###########################################\n",
    "        \n",
    "    ###########################################\n",
    "    #@TODO: Enter Code Here - add a name scope\n",
    "    #to train_step. name it \"train\"\n",
    "    with tf.name_scope(\"train\"):\n",
    "    ###########################################\n",
    "    \n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    ###########################################\n",
    "    #@TODO: Enter Code Here - add a name scope\n",
    "    #to prediction and accuracy. name it \n",
    "    #\"accuracy\"\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "    ###########################################\n",
    "    \n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        ###########################################\n",
    "        #@TODO: Enter Code Here - add scalar\n",
    "        # summaries to accuracy, with name \"accuracy\"\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        ###########################################\n",
    "        \n",
    "    # merge all summaries    \n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    # Embedding visualization\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), \n",
    "                            name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(log_dir + \"/\" + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = MNIST_FOLDER + '/sprite_1024.png'\n",
    "    embedding_config.metadata_path = MNIST_FOLDER + '/labels_1024.tsv'\n",
    "      \n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    for i in range(num_iterations+1):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        if i % 10 == 0:\n",
    "            [loss, train_accuracy, s] = sess.run([xent, accuracy, summ], \n",
    "                                           feed_dict={x: batch[0], \n",
    "                                                      y: batch[1],\n",
    "                                                     keep_prob: 1.0})\n",
    "            print(\"step %d, minibatch loss %g, training accuracy %g\" \n",
    "                  % (i, loss, train_accuracy))\n",
    "            writer.add_summary(s, i)\n",
    "        train_step.run(session=sess, feed_dict={x: batch[0], \n",
    "                                                y: batch[1], \n",
    "                                                keep_prob: 0.5})\n",
    "        if i % 100 == 0:\n",
    "            sess.run(assignment, feed_dict={x: mnist.test.images[:1024], \n",
    "                                            y: mnist.test.labels[:1024],\n",
    "                                           keep_prob: 0.5})\n",
    "            saver.save(sess, os.path.join(log_dir, \"model.ckpt\"), i)\n",
    "            print('test accuracy %g' % accuracy.eval(session=sess, feed_dict={\n",
    "                x: mnist.test.images, \n",
    "                y: mnist.test.labels, \n",
    "                keep_prob: 1.0}))\n",
    "\n",
    "def make_hparam_string(learning_rate, \n",
    "                       use_two_fc, \n",
    "                       use_two_conv,\n",
    "                       num_iterations,\n",
    "                       batch_size):\n",
    "    conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "    num_iteration_param = \"num_iter=\" + str(num_iterations)\n",
    "    batch_size_param = \"batch_size=\" + str(batch_size)\n",
    "    return \"lr_%.0E,%s,%s,%s,%s\" % (learning_rate, conv_param, \n",
    "                                    fc_param, num_iteration_param,\n",
    "                                   batch_size_param)\n",
    "\n",
    "\n",
    "for learning_rate in [1E-4, 1E-3]:\n",
    "    # Include \"False\" as a value to try different model architectures\n",
    "    for use_two_fc in [True, False]:\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "          for use_two_conv in [True, False]:\n",
    "            # Include different batch size\n",
    "            for batch_size in [100]:\n",
    "                # Include different number of iterations\n",
    "                for num_iterations in [200]:\n",
    "                    \n",
    "                    hparam = make_hparam_string(learning_rate, use_two_fc, \n",
    "                                                use_two_conv, \n",
    "                                                num_iterations, \n",
    "                                                batch_size)\n",
    "                    print('Starting run for %s' % hparam)\n",
    "\n",
    "                    # Actually run with the new settings\n",
    "                    mnist_model(learning_rate, use_two_fc, use_two_conv, \n",
    "                                num_iterations, batch_size, hparam, TRAIN_3_FOLDER)\n",
    "                    \n",
    "                    \n",
    "#Setting up tensorboard and running on anaconda\n",
    "\"\"\"\n",
    "tensorboard --logdir=/Users/luna/workspace/TensorFlow-Tutorial/tf_mnist/train-2\n",
    "Click on Graphs tab:\n",
    "On the left hand side, you will see a field called \"Run\"\n",
    "Select the \"lr_1E-04,conv=2,fc=2,2000,batch_size=100\" run\n",
    "\n",
    "About each Visualization on Tensorboard:\n",
    "Weights Histogram: The historgram looks like a normal distribution. With increasing time steps, the distribution seems to approach a bell curve. This might signify that our model is generalizing well.\n",
    "\n",
    "Biases Histogram: It is seen that at early stages of the training, the biases are big and it decreases gradually with increasing time steps.\n",
    "\n",
    "Activations Histogram: It is seen that the activation histogram has a spike at bin value of 0.0934 which signifies the threshold of our neuron.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
